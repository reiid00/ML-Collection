{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config, TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import pipeline\n",
    "import optuna\n",
    "from transformers import EarlyStoppingCallback\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOKS_PATH = \"processed_books.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_data(file_path, test_size=0.1):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    stories = text.split(\"\\n\\n\")\n",
    "    train_stories, test_stories = train_test_split(stories, test_size=test_size, random_state=42)\n",
    "\n",
    "    return train_stories, test_stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stories, test_stories = load_and_split_data(BOOKS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = \"train_dataset.txt\"\n",
    "test_file_path = \"test_dataset.txt\"\n",
    "\n",
    "with open(train_file_path, \"w\") as file:\n",
    "    file.write(\"\\n\\n\".join(train_stories))\n",
    "with open(test_file_path, \"w\") as file:\n",
    "    file.write(\"\\n\\n\".join(test_stories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=train_file_path,\n",
    "    block_size=128\n",
    ")\n",
    "\n",
    "test_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=test_file_path,\n",
    "    block_size=128\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False # If set to False, the labels are the same as the inputs with the padding tokens ignored\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, train_dataset,test_dataset, data_collator):\n",
    "\n",
    "    # Define hyperparameters using trial.suggest_* methods\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
    "    per_device_train_batch_size = trial.suggest_categorical(\"per_device_train_batch_size\", [4, 8, 16])\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0, 0.1)\n",
    "    warmup_steps = trial.suggest_int(\"warmup_steps\", 0, 500)\n",
    "    epochs = 3\n",
    "\n",
    "    # Set up the training arguments using the suggested hyperparameters\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./output\",\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        warmup_steps=warmup_steps,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=500,\n",
    "        eval_steps=500,\n",
    "        save_steps=500,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"loss\",\n",
    "        greater_is_better=False,\n",
    "        report_to=[\"tensorboard\"],\n",
    "        seed=42,\n",
    "        disable_tqdm=True,\n",
    "    )\n",
    "\n",
    "    # Set up the Trainer instance using the suggested hyperparameters\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    )\n",
    "\n",
    "    # Train the model and return the best loss\n",
    "    trainer.train()\n",
    "    best_loss = trainer.evaluate()[\"eval_loss\"]\n",
    "    return best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 1020\n",
      "Evaluation dataset size: 98\n"
     ]
    }
   ],
   "source": [
    "# Take only 10% of the train and test datasets so it's faster to hyper parameter tune\n",
    "train_dataset_hpt = train_dataset[:int(len(train_dataset) * 0.01)]\n",
    "eval_dataset_hpt= test_dataset[:int(len(test_dataset) * 0.01)]\n",
    "\n",
    "print(\"Training dataset size:\", len(train_dataset_hpt))\n",
    "print(\"Evaluation dataset size:\", len(eval_dataset_hpt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-17 17:26:25,762]\u001b[0m A new study created in memory with name: gpt2_hyperparameter_tuning\u001b[0m\n",
      "C:\\Users\\reidp\\AppData\\Local\\Temp\\ipykernel_19884\\491586242.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
      "c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 81.5046, 'train_samples_per_second': 37.544, 'train_steps_per_second': 2.356, 'train_loss': 4.65250809987386, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-17 17:27:49,001]\u001b[0m Trial 0 finished with value: 5.920174598693848 and parameters: {'learning_rate': 3.4073154032402004e-05, 'per_device_train_batch_size': 16, 'weight_decay': 0.02363315374328481, 'warmup_steps': 474}. Best is trial 0 with value: 5.920174598693848.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.920174598693848, 'eval_runtime': 0.5589, 'eval_samples_per_second': 175.358, 'eval_steps_per_second': 23.262, 'epoch': 3.0}\n",
      "{'loss': 4.1135, 'learning_rate': 6.803764396982683e-05, 'epoch': 1.96}\n",
      "{'eval_loss': 6.6518659591674805, 'eval_runtime': 0.5905, 'eval_samples_per_second': 165.951, 'eval_steps_per_second': 22.014, 'epoch': 1.96}\n",
      "{'train_runtime': 86.3183, 'train_samples_per_second': 35.45, 'train_steps_per_second': 8.863, 'train_loss': 3.901376203150531, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-17 17:29:15,943]\u001b[0m Trial 1 finished with value: 6.6518659591674805 and parameters: {'learning_rate': 0.00010680626374131307, 'per_device_train_batch_size': 4, 'weight_decay': 0.023937529176263652, 'warmup_steps': 349}. Best is trial 0 with value: 5.920174598693848.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.6518659591674805, 'eval_runtime': 0.5825, 'eval_samples_per_second': 168.232, 'eval_steps_per_second': 22.316, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reidp\\AppData\\Local\\Temp\\ipykernel_19884\\491586242.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
      "c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.479, 'learning_rate': 0.00053605656719458, 'epoch': 1.96}\n",
      "{'eval_loss': 8.383543014526367, 'eval_runtime': 0.5715, 'eval_samples_per_second': 171.473, 'eval_steps_per_second': 22.746, 'epoch': 1.96}\n",
      "{'train_runtime': 84.8371, 'train_samples_per_second': 36.069, 'train_steps_per_second': 9.017, 'train_loss': 3.2747329612183416, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-17 17:30:41,383]\u001b[0m Trial 2 finished with value: 8.383543014526367 and parameters: {'learning_rate': 0.0005401022771356712, 'per_device_train_batch_size': 4, 'weight_decay': 0.036951970829043744, 'warmup_steps': 498}. Best is trial 0 with value: 5.920174598693848.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.383543014526367, 'eval_runtime': 0.5835, 'eval_samples_per_second': 167.943, 'eval_steps_per_second': 22.278, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reidp\\AppData\\Local\\Temp\\ipykernel_19884\\491586242.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
      "c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"gpt2_hyperparameter_tuning\")\n",
    "\n",
    "# Optimize the hyperparameters\n",
    "study.optimize(lambda trial: objective(trial, train_dataset_hpt, eval_dataset_hpt, data_collator), n_trials=20, timeout=3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
