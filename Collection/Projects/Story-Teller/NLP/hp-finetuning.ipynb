{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel,  TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import optuna\n",
    "from transformers import EarlyStoppingCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOKS_PATH = \"processed_books.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_data(file_path, test_size=0.1):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    stories = text.split(\"\\n\\n\")\n",
    "    train_stories, test_stories = train_test_split(stories, test_size=test_size, random_state=42)\n",
    "\n",
    "    return train_stories, test_stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stories, test_stories = load_and_split_data(BOOKS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = \"train_dataset.txt\"\n",
    "test_file_path = \"test_dataset.txt\"\n",
    "\n",
    "with open(train_file_path, \"w\") as file:\n",
    "    file.write(\"\\n\\n\".join(train_stories))\n",
    "with open(test_file_path, \"w\") as file:\n",
    "    file.write(\"\\n\\n\".join(test_stories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=train_file_path,\n",
    "    block_size=128\n",
    ")\n",
    "\n",
    "test_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=test_file_path,\n",
    "    block_size=128\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False # If set to False, the labels are the same as the inputs with the padding tokens ignored\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, train_dataset,test_dataset, data_collator):\n",
    "\n",
    "    # Define hyperparameters using trial.suggest_* methods\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
    "    per_device_train_batch_size = trial.suggest_categorical(\"per_device_train_batch_size\", [4, 8, 16])\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0, 0.1)\n",
    "    warmup_steps = trial.suggest_int(\"warmup_steps\", 0, 500)\n",
    "    epochs = 3\n",
    "\n",
    "    # Set up the training arguments using the suggested hyperparameters\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./output\",\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        warmup_steps=warmup_steps,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=500,\n",
    "        eval_steps=500,\n",
    "        save_steps=500,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"loss\",\n",
    "        greater_is_better=False,\n",
    "        report_to=[\"tensorboard\"],\n",
    "        seed=42,\n",
    "        disable_tqdm=True,\n",
    "    )\n",
    "\n",
    "    # Set up the Trainer instance using the suggested hyperparameters\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    )\n",
    "\n",
    "    # Train the model and return the best loss\n",
    "    trainer.train()\n",
    "    best_loss = trainer.evaluate()[\"eval_loss\"]\n",
    "    return best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 1020\n",
      "Evaluation dataset size: 98\n"
     ]
    }
   ],
   "source": [
    "# Take only 1% of the train and test datasets so it's faster to hyper parameter tune\n",
    "train_dataset_hpt = train_dataset[:int(len(train_dataset) * 0.01)]\n",
    "eval_dataset_hpt= test_dataset[:int(len(test_dataset) * 0.01)]\n",
    "\n",
    "print(\"Training dataset size:\", len(train_dataset_hpt))\n",
    "print(\"Evaluation dataset size:\", len(eval_dataset_hpt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-17 17:32:51,746]\u001b[0m A new study created in memory with name: gpt2_hyperparameter_tuning\u001b[0m\n",
      "C:\\Users\\reidp\\AppData\\Local\\Temp\\ipykernel_18476\\491586242.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
      "c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 67.7811, 'train_samples_per_second': 45.145, 'train_steps_per_second': 5.665, 'train_loss': 4.422989209493001, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-17 17:34:00,551]\u001b[0m Trial 0 finished with value: 6.1424336433410645 and parameters: {'learning_rate': 2.419197584455757e-05, 'per_device_train_batch_size': 8, 'weight_decay': 0.08044739840132131, 'warmup_steps': 229}. Best is trial 0 with value: 6.1424336433410645.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.1424336433410645, 'eval_runtime': 0.5595, 'eval_samples_per_second': 175.154, 'eval_steps_per_second': 23.235, 'epoch': 3.0}\n",
      "{'loss': 4.0706, 'learning_rate': 1.0999455852539922e-05, 'epoch': 1.96}\n",
      "{'eval_loss': 6.32277250289917, 'eval_runtime': 0.5505, 'eval_samples_per_second': 178.02, 'eval_steps_per_second': 23.615, 'epoch': 1.96}\n",
      "{'train_runtime': 88.1986, 'train_samples_per_second': 34.694, 'train_steps_per_second': 8.674, 'train_loss': 4.037008527369281, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-17 17:35:29,326]\u001b[0m Trial 1 finished with value: 6.32277250289917 and parameters: {'learning_rate': 1.1165485374842412e-05, 'per_device_train_batch_size': 4, 'weight_decay': 0.02402148423109477, 'warmup_steps': 496}. Best is trial 0 with value: 6.1424336433410645.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.32277250289917, 'eval_runtime': 0.5545, 'eval_samples_per_second': 176.735, 'eval_steps_per_second': 23.444, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reidp\\AppData\\Local\\Temp\\ipykernel_18476\\491586242.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
      "c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0116, 'learning_rate': 0.000318441699148407, 'epoch': 1.96}\n",
      "{'eval_loss': 8.05346393585205, 'eval_runtime': 0.5705, 'eval_samples_per_second': 171.773, 'eval_steps_per_second': 22.786, 'epoch': 1.96}\n",
      "{'train_runtime': 97.1812, 'train_samples_per_second': 31.488, 'train_steps_per_second': 7.872, 'train_loss': 3.5803996566074345, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-17 17:37:07,088]\u001b[0m Trial 2 finished with value: 8.05346393585205 and parameters: {'learning_rate': 0.0004674483810140767, 'per_device_train_batch_size': 4, 'weight_decay': 0.050548471122534556, 'warmup_steps': 376}. Best is trial 0 with value: 6.1424336433410645.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.05346393585205, 'eval_runtime': 0.5635, 'eval_samples_per_second': 173.91, 'eval_steps_per_second': 23.07, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reidp\\AppData\\Local\\Temp\\ipykernel_18476\\491586242.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
      "c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3553, 'learning_rate': 1.4887265633513327e-05, 'epoch': 1.96}\n",
      "{'eval_loss': 9.513703346252441, 'eval_runtime': 0.5625, 'eval_samples_per_second': 174.219, 'eval_steps_per_second': 23.111, 'epoch': 1.96}\n",
      "{'train_runtime': 96.2687, 'train_samples_per_second': 31.786, 'train_steps_per_second': 7.947, 'train_loss': 2.3999334597120097, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-17 17:38:43,932]\u001b[0m Trial 3 finished with value: 9.513703346252441 and parameters: {'learning_rate': 2.9212747658214834e-05, 'per_device_train_batch_size': 4, 'weight_decay': 0.08238532313777838, 'warmup_steps': 245}. Best is trial 0 with value: 6.1424336433410645.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.513703346252441, 'eval_runtime': 0.5555, 'eval_samples_per_second': 176.416, 'eval_steps_per_second': 23.402, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reidp\\AppData\\Local\\Temp\\ipykernel_18476\\491586242.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
      "c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8543, 'learning_rate': 7.552993934050345e-05, 'epoch': 1.96}\n",
      "{'eval_loss': 9.801215171813965, 'eval_runtime': 0.5695, 'eval_samples_per_second': 172.076, 'eval_steps_per_second': 22.826, 'epoch': 1.96}\n",
      "{'train_runtime': 97.7069, 'train_samples_per_second': 31.318, 'train_steps_per_second': 7.83, 'train_loss': 1.8497486787683823, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-17 17:40:22,212]\u001b[0m Trial 4 finished with value: 9.801215171813965 and parameters: {'learning_rate': 0.00015191493459806922, 'per_device_train_batch_size': 4, 'weight_decay': 0.042782361811039483, 'warmup_steps': 232}. Best is trial 0 with value: 6.1424336433410645.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.801215171813965, 'eval_runtime': 0.5545, 'eval_samples_per_second': 176.735, 'eval_steps_per_second': 23.444, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reidp\\AppData\\Local\\Temp\\ipykernel_18476\\491586242.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
      "c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 54.1663, 'train_samples_per_second': 56.493, 'train_steps_per_second': 3.545, 'train_loss': 1.4974150657653809, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-17 17:41:16,967]\u001b[0m Trial 5 finished with value: 12.223514556884766 and parameters: {'learning_rate': 0.0006319102804218298, 'per_device_train_batch_size': 16, 'weight_decay': 0.05450772149369192, 'warmup_steps': 20}. Best is trial 0 with value: 6.1424336433410645.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 12.223514556884766, 'eval_runtime': 0.5725, 'eval_samples_per_second': 171.173, 'eval_steps_per_second': 22.707, 'epoch': 3.0}\n",
      "{'train_runtime': 63.3905, 'train_samples_per_second': 48.272, 'train_steps_per_second': 6.058, 'train_loss': 0.6961462497711182, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-17 17:42:20,934]\u001b[0m Trial 6 finished with value: 12.589221000671387 and parameters: {'learning_rate': 3.640417825754544e-05, 'per_device_train_batch_size': 8, 'weight_decay': 0.09138263381858094, 'warmup_steps': 226}. Best is trial 0 with value: 6.1424336433410645.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 12.589221000671387, 'eval_runtime': 0.5585, 'eval_samples_per_second': 175.468, 'eval_steps_per_second': 23.276, 'epoch': 3.0}\n",
      "{'train_runtime': 54.4315, 'train_samples_per_second': 56.217, 'train_steps_per_second': 3.527, 'train_loss': 0.34238115946451825, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-17 17:43:15,942]\u001b[0m Trial 7 finished with value: 12.722526550292969 and parameters: {'learning_rate': 1.6625621374717557e-05, 'per_device_train_batch_size': 16, 'weight_decay': 0.0694585179185626, 'warmup_steps': 260}. Best is trial 0 with value: 6.1424336433410645.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 12.722526550292969, 'eval_runtime': 0.5595, 'eval_samples_per_second': 175.154, 'eval_steps_per_second': 23.235, 'epoch': 3.0}\n",
      "{'train_runtime': 63.4606, 'train_samples_per_second': 48.219, 'train_steps_per_second': 6.051, 'train_loss': 0.4814045826594035, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-17 17:44:19,973]\u001b[0m Trial 8 finished with value: 12.823318481445312 and parameters: {'learning_rate': 3.815339443127858e-05, 'per_device_train_batch_size': 8, 'weight_decay': 0.07584231388822649, 'warmup_steps': 363}. Best is trial 0 with value: 6.1424336433410645.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 12.823318481445312, 'eval_runtime': 0.5545, 'eval_samples_per_second': 176.735, 'eval_steps_per_second': 23.444, 'epoch': 3.0}\n",
      "{'loss': 0.293, 'learning_rate': 6.301315496715391e-05, 'epoch': 1.96}\n",
      "{'eval_loss': 13.011134147644043, 'eval_runtime': 0.5575, 'eval_samples_per_second': 175.783, 'eval_steps_per_second': 23.318, 'epoch': 1.96}\n",
      "{'train_runtime': 94.8803, 'train_samples_per_second': 32.251, 'train_steps_per_second': 8.063, 'train_loss': 0.3642796934040543, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-17 17:45:55,427]\u001b[0m Trial 9 finished with value: 13.011134147644043 and parameters: {'learning_rate': 6.895779222820616e-05, 'per_device_train_batch_size': 4, 'weight_decay': 0.0898280171567001, 'warmup_steps': 475}. Best is trial 0 with value: 6.1424336433410645.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 13.011134147644043, 'eval_runtime': 0.5545, 'eval_samples_per_second': 176.734, 'eval_steps_per_second': 23.444, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reidp\\AppData\\Local\\Temp\\ipykernel_18476\\491586242.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
      "c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 63.3306, 'train_samples_per_second': 48.318, 'train_steps_per_second': 6.063, 'train_loss': 0.29982878764470416, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-17 17:46:59,380]\u001b[0m Trial 10 finished with value: 13.078715324401855 and parameters: {'learning_rate': 1.0462223523755512e-05, 'per_device_train_batch_size': 8, 'weight_decay': 0.0004677975950316479, 'warmup_steps': 99}. Best is trial 0 with value: 6.1424336433410645.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 13.078715324401855, 'eval_runtime': 0.5685, 'eval_samples_per_second': 172.378, 'eval_steps_per_second': 22.867, 'epoch': 3.0}\n",
      "{'train_runtime': 63.3736, 'train_samples_per_second': 48.285, 'train_steps_per_second': 6.059, 'train_loss': 0.2543323834737142, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-17 17:48:03,338]\u001b[0m Trial 11 finished with value: 13.118705749511719 and parameters: {'learning_rate': 1.043664646752327e-05, 'per_device_train_batch_size': 8, 'weight_decay': 0.025253735353051192, 'warmup_steps': 490}. Best is trial 0 with value: 6.1424336433410645.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 13.118705749511719, 'eval_runtime': 0.5575, 'eval_samples_per_second': 175.783, 'eval_steps_per_second': 23.318, 'epoch': 3.0}\n",
      "{'train_runtime': 63.53, 'train_samples_per_second': 48.166, 'train_steps_per_second': 6.044, 'train_loss': 0.22252478202184042, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-17 17:49:07,452]\u001b[0m Trial 12 finished with value: 13.190448760986328 and parameters: {'learning_rate': 1.9995152580558703e-05, 'per_device_train_batch_size': 8, 'weight_decay': 0.09721050432716691, 'warmup_steps': 150}. Best is trial 0 with value: 6.1424336433410645.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 13.190448760986328, 'eval_runtime': 0.5555, 'eval_samples_per_second': 176.416, 'eval_steps_per_second': 23.402, 'epoch': 3.0}\n",
      "{'loss': 0.1326, 'learning_rate': 1.1943399667858717e-05, 'epoch': 1.96}\n",
      "{'eval_loss': 13.292131423950195, 'eval_runtime': 0.5585, 'eval_samples_per_second': 175.468, 'eval_steps_per_second': 23.276, 'epoch': 1.96}\n",
      "{'train_runtime': 86.4298, 'train_samples_per_second': 35.404, 'train_steps_per_second': 8.851, 'train_loss': 0.2264634176017412, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-17 17:50:34,470]\u001b[0m Trial 13 finished with value: 13.292131423950195 and parameters: {'learning_rate': 1.9469994930245154e-05, 'per_device_train_batch_size': 4, 'weight_decay': 0.06743021207814394, 'warmup_steps': 333}. Best is trial 0 with value: 6.1424336433410645.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 13.292131423950195, 'eval_runtime': 0.5575, 'eval_samples_per_second': 175.783, 'eval_steps_per_second': 23.318, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reidp\\AppData\\Local\\Temp\\ipykernel_18476\\491586242.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
      "c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 54.3462, 'train_samples_per_second': 56.306, 'train_steps_per_second': 3.533, 'train_loss': 0.16834348440170288, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-17 17:51:29,412]\u001b[0m Trial 14 finished with value: 13.191934585571289 and parameters: {'learning_rate': 5.9320832480171374e-05, 'per_device_train_batch_size': 16, 'weight_decay': 0.029653036899733642, 'warmup_steps': 412}. Best is trial 0 with value: 6.1424336433410645.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 13.191934585571289, 'eval_runtime': 0.5675, 'eval_samples_per_second': 172.683, 'eval_steps_per_second': 22.907, 'epoch': 3.0}\n",
      "{'train_runtime': 63.3928, 'train_samples_per_second': 48.27, 'train_steps_per_second': 6.057, 'train_loss': 0.1598572830359141, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-17 17:52:33,386]\u001b[0m Trial 15 finished with value: 13.278518676757812 and parameters: {'learning_rate': 1.0733924552779137e-05, 'per_device_train_batch_size': 8, 'weight_decay': 0.06347038188376201, 'warmup_steps': 143}. Best is trial 0 with value: 6.1424336433410645.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 13.278518676757812, 'eval_runtime': 0.5525, 'eval_samples_per_second': 177.375, 'eval_steps_per_second': 23.529, 'epoch': 3.0}\n",
      "{'loss': 0.1054, 'learning_rate': 6.938189126122517e-05, 'epoch': 1.96}\n",
      "{'eval_loss': 13.466434478759766, 'eval_runtime': 0.6273, 'eval_samples_per_second': 156.231, 'eval_steps_per_second': 20.725, 'epoch': 1.96}\n",
      "{'train_runtime': 96.3087, 'train_samples_per_second': 31.773, 'train_steps_per_second': 7.943, 'train_loss': 0.20075369940863716, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-17 17:54:10,302]\u001b[0m Trial 16 finished with value: 13.466434478759766 and parameters: {'learning_rate': 0.00012043649049118331, 'per_device_train_batch_size': 4, 'weight_decay': 0.09866938284920393, 'warmup_steps': 305}. Best is trial 0 with value: 6.1424336433410645.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 13.466434478759766, 'eval_runtime': 0.5753, 'eval_samples_per_second': 170.344, 'eval_steps_per_second': 22.597, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reidp\\AppData\\Local\\Temp\\ipykernel_18476\\491586242.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
      "c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 64.993, 'train_samples_per_second': 47.082, 'train_steps_per_second': 5.908, 'train_loss': 0.14672996600468954, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-17 17:55:15,896]\u001b[0m Trial 17 finished with value: 13.509056091308594 and parameters: {'learning_rate': 2.2289177387709766e-05, 'per_device_train_batch_size': 8, 'weight_decay': 0.08110148619051243, 'warmup_steps': 430}. Best is trial 0 with value: 6.1424336433410645.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 13.509056091308594, 'eval_runtime': 0.5725, 'eval_samples_per_second': 171.173, 'eval_steps_per_second': 22.707, 'epoch': 3.0}\n",
      "{'loss': 0.0532, 'learning_rate': 7.094769568696163e-06, 'epoch': 1.96}\n",
      "{'eval_loss': 13.622440338134766, 'eval_runtime': 0.5775, 'eval_samples_per_second': 169.69, 'eval_steps_per_second': 22.51, 'epoch': 1.96}\n",
      "{'train_runtime': 86.7103, 'train_samples_per_second': 35.29, 'train_steps_per_second': 8.822, 'train_loss': 0.1414006526174109, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-17 17:56:43,210]\u001b[0m Trial 18 finished with value: 13.622440338134766 and parameters: {'learning_rate': 1.584944748931369e-05, 'per_device_train_batch_size': 4, 'weight_decay': 0.037885420394616086, 'warmup_steps': 173}. Best is trial 0 with value: 6.1424336433410645.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 13.622440338134766, 'eval_runtime': 0.5735, 'eval_samples_per_second': 170.874, 'eval_steps_per_second': 22.667, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reidp\\AppData\\Local\\Temp\\ipykernel_18476\\491586242.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3)\n",
      "c:\\Users\\reidp\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 54.6698, 'train_samples_per_second': 55.972, 'train_steps_per_second': 3.512, 'train_loss': 0.09289287527402242, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-17 17:57:38,566]\u001b[0m Trial 19 finished with value: 13.563915252685547 and parameters: {'learning_rate': 4.591045607150248e-05, 'per_device_train_batch_size': 16, 'weight_decay': 0.06072850400850554, 'warmup_steps': 47}. Best is trial 0 with value: 6.1424336433410645.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 13.563915252685547, 'eval_runtime': 0.6551, 'eval_samples_per_second': 149.595, 'eval_steps_per_second': 19.844, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"gpt2_hyperparameter_tuning\")\n",
    "\n",
    "# Optimize the hyperparameters\n",
    "study.optimize(lambda trial: objective(trial, train_dataset_hpt, eval_dataset_hpt, data_collator), n_trials=20, timeout=3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best hyperparameters to a JSON file\n",
    "with open(\"best_hyperparams.json\", \"w\") as f:\n",
    "    json.dump(best_params, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
